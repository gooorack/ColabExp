{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Andrew_TextAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RICC8aT-s52h"
      },
      "source": [
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have two advantages:\n",
        "\n",
        "we don't have to worry about text preprocessing,\n",
        "we can benefit from transfer learning.\n",
        "Using Model --> TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT1HV-PA4TW6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Set Tensorflow Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r65VDDX1JWUC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0a06efb-ec58-4fb1-e2e5-5a83eaee6593"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4frgIccIJbMu"
      },
      "source": [
        "! is used because its a command line command and not a python command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b50DFktAJPgi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "19251d99-bd4a-4b51-89d2-b64af02622de"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6twch-kpJWLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "641f896e-2052-4ecd-a301-2d6203f2d28c"
      },
      "source": [
        "\n",
        "import tensorflow as tf \n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gD_HJ5aWNzA"
      },
      "source": [
        "Read in Data and allocate to test and training sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzwz5o6PJWW8"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "#files.upload()                           #Uncomment this to enable in browser file upload\n",
        "training_data = pd.read_csv(\"training_data.csv\",sep='|',header=None)\n",
        "test_data = pd.read_csv(\"test_data.csv\",sep='|',header=None)\n",
        "\n",
        "#remove headers\n",
        "training_data[0].pop(0)\n",
        "training_data[1].pop(0)\n",
        "test_data[0].pop(0)\n",
        "test_data[1].pop(0)\n",
        "\n",
        "#questions\n",
        "print(\"Before Shuffle - questions\")\n",
        "for a in training_data[0][:10]:\n",
        "    print(a)\n",
        "\n",
        "#answers\n",
        "print(\"Before Shuffle - answers\")\n",
        "for a in training_data[1][:10]:\n",
        "    print(a)\n",
        "\n",
        "#Shuffle input\n",
        "training_data = training_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#questions\n",
        "print(\"After Shuffle - questions\")\n",
        "for a in training_data[0][:10]:\n",
        "    print(a)\n",
        "\n",
        "#answers\n",
        "print(\"After Shuffle - answers\")\n",
        "for a in training_data[1][:10]:\n",
        "    print(a)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvlGU_woYJmv"
      },
      "source": [
        "#split into Training, Validation and Test.\n",
        "train_examples = training_data[0]\n",
        "train_labels = training_data[1]\n",
        "\n",
        "test_examples = test_data[0]\n",
        "test_labels = test_data[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSnCvEGr2dKy"
      },
      "source": [
        "#Convert Labels Catagories into\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit_transform(train_labels)\n",
        "train_labels_int = encoder.transform(train_labels)\n",
        "test_labels_int = encoder.transform(test_labels)\n",
        "num_classes = len(encoder.classes_)\n",
        "\n",
        "# Print all possible genres and the labels for the first movie in our training dataset\n",
        "print(encoder.classes_)\n",
        "\n",
        "for a in train_labels[:10]:\n",
        "    print(a)\n",
        "\n",
        "for a in train_labels_int[:10]:\n",
        "    print(a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RG8ouQmozHv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e0913270-9f3a-4950-8284-8fb3ebd8e679"
      },
      "source": [
        "#import existing trained NLP Model\n",
        "#converts from a sentence into a 20 dimention vector.\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(model, output_shape=[20], input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples[:3])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
              "array([[ 0.7767494 , -1.3949643 ,  1.8822532 ,  0.5356442 , -1.0081346 ,\n",
              "        -2.2011378 , -0.04300665,  0.27778423, -0.26042593,  1.3977667 ,\n",
              "        -0.03493638, -0.74132186, -0.08866705,  0.6111342 , -2.9438055 ,\n",
              "        -0.3955079 ,  3.248432  , -0.95043993, -1.321964  , -0.00502632],\n",
              "       [ 0.371193  , -1.1949918 ,  1.9071492 ,  0.36187264,  0.24854447,\n",
              "        -1.595161  ,  0.5150494 , -0.65282017, -0.7913214 ,  0.66825783,\n",
              "         0.68587404, -0.6499917 , -0.14705275,  0.80869836, -2.0821605 ,\n",
              "        -0.71447015,  3.2234712 , -1.6162708 , -0.9779024 ,  0.20788188],\n",
              "       [ 0.712013  , -1.6389797 ,  1.901473  ,  1.5259099 , -1.5908127 ,\n",
              "        -2.4464293 ,  0.09874652,  0.7626246 , -0.27520013,  0.15411198,\n",
              "        -0.9386238 ,  0.3088036 ,  0.02570286,  0.84749335, -0.97850776,\n",
              "        -0.217274  ,  2.667848  , -0.39907122, -0.29446137, -1.3394673 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyzDmMalvF_v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3d8c733d-a021-454f-cb97-34b10c8d5802"
      },
      "source": [
        "#Build the rest of the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))         #1 hidden layer\n",
        "model.add(tf.keras.layers.Dense(5, activation='softmax'))     #was sigmoid\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_2 (KerasLayer)   (None, 20)                400020    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 85        \n",
            "=================================================================\n",
            "Total params: 400,441\n",
            "Trainable params: 400,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj8a6qIAw_Cp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5cd29fe0-dc28-4509-fc0c-2393a9f04581"
      },
      "source": [
        "#Create a validation Set\n",
        "x_val = train_examples[:50]\n",
        "x_train = train_examples#[50:]\n",
        "\n",
        "y_val = train_labels_int[:50]\n",
        "y_train = train_labels_int#[50:]\n",
        "\n",
        "test_examples = test_data[0]\n",
        "test_labels = test_data[1]\n",
        "\n",
        "import numpy as np\n",
        "y_val = np.array(y_val)\n",
        "x_val = np.array(x_val)\n",
        "y_train = np.array(y_train)\n",
        "x_train = np.array(x_train)\n",
        "test_examples = np.array(test_examples)\n",
        "test_labels =  np.array(test_labels)\n",
        "\n",
        "print (\"x validation: \" , len(x_val))\n",
        "print (\"x train: \" , len(x_train))\n",
        "print (\"y validation: \" , len(y_val))\n",
        "print (\"y train: \" , len(y_train))\n",
        "print (\"x test: \" , len(test_examples))\n",
        "print (\"y test: \" , len(test_labels_int))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x validation:  50\n",
            "x train:  280\n",
            "y validation:  50\n",
            "y train:  280\n",
            "x test:  21\n",
            "y test:  21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP5N79bgxZ4g"
      },
      "source": [
        "#set remaining parameters\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYDauMwaxM3B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bce6a38-222e-40c8-8e65-334e153bec9f"
      },
      "source": [
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=50,\n",
        "                    validation_data=(x_val, y_val),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 280 samples, validate on 50 samples\n",
            "Epoch 1/40\n",
            "280/280 [==============================] - 0s 2ms/sample - loss: 0.0301 - accuracy: 0.9964 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
            "Epoch 2/40\n",
            "280/280 [==============================] - 0s 188us/sample - loss: 0.0262 - accuracy: 0.9964 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 3/40\n",
            "280/280 [==============================] - 0s 235us/sample - loss: 0.0233 - accuracy: 0.9964 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "Epoch 4/40\n",
            "280/280 [==============================] - 0s 185us/sample - loss: 0.0209 - accuracy: 0.9964 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 5/40\n",
            "280/280 [==============================] - 0s 201us/sample - loss: 0.0188 - accuracy: 0.9964 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 6/40\n",
            "280/280 [==============================] - 0s 197us/sample - loss: 0.0171 - accuracy: 0.9964 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 7/40\n",
            "280/280 [==============================] - 0s 184us/sample - loss: 0.0157 - accuracy: 0.9964 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 8/40\n",
            "280/280 [==============================] - 0s 189us/sample - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 9/40\n",
            "280/280 [==============================] - 0s 190us/sample - loss: 0.0134 - accuracy: 0.9964 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 10/40\n",
            "280/280 [==============================] - 0s 195us/sample - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 11/40\n",
            "280/280 [==============================] - 0s 192us/sample - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 12/40\n",
            "280/280 [==============================] - 0s 187us/sample - loss: 0.0109 - accuracy: 0.9964 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 13/40\n",
            "280/280 [==============================] - 0s 196us/sample - loss: 0.0102 - accuracy: 0.9964 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 14/40\n",
            "280/280 [==============================] - 0s 213us/sample - loss: 0.0096 - accuracy: 0.9964 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 15/40\n",
            "280/280 [==============================] - 0s 203us/sample - loss: 0.0091 - accuracy: 0.9964 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 16/40\n",
            "280/280 [==============================] - 0s 181us/sample - loss: 0.0085 - accuracy: 0.9964 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 17/40\n",
            "280/280 [==============================] - 0s 191us/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 18/40\n",
            "280/280 [==============================] - 0s 186us/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 19/40\n",
            "280/280 [==============================] - 0s 205us/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 20/40\n",
            "280/280 [==============================] - 0s 177us/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 21/40\n",
            "280/280 [==============================] - 0s 205us/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 22/40\n",
            "280/280 [==============================] - 0s 188us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 23/40\n",
            "280/280 [==============================] - 0s 197us/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 24/40\n",
            "280/280 [==============================] - 0s 275us/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 25/40\n",
            "280/280 [==============================] - 0s 206us/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 26/40\n",
            "280/280 [==============================] - 0s 196us/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 27/40\n",
            "280/280 [==============================] - 0s 204us/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 28/40\n",
            "280/280 [==============================] - 0s 196us/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 29/40\n",
            "280/280 [==============================] - 0s 179us/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 30/40\n",
            "280/280 [==============================] - 0s 194us/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 31/40\n",
            "280/280 [==============================] - 0s 176us/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 9.7507e-04 - val_accuracy: 1.0000\n",
            "Epoch 32/40\n",
            "280/280 [==============================] - 0s 204us/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 9.3133e-04 - val_accuracy: 1.0000\n",
            "Epoch 33/40\n",
            "280/280 [==============================] - 0s 211us/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 8.9429e-04 - val_accuracy: 1.0000\n",
            "Epoch 34/40\n",
            "280/280 [==============================] - 0s 193us/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 8.5280e-04 - val_accuracy: 1.0000\n",
            "Epoch 35/40\n",
            "280/280 [==============================] - 0s 202us/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 8.1542e-04 - val_accuracy: 1.0000\n",
            "Epoch 36/40\n",
            "280/280 [==============================] - 0s 200us/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 7.8296e-04 - val_accuracy: 1.0000\n",
            "Epoch 37/40\n",
            "280/280 [==============================] - 0s 190us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 7.5395e-04 - val_accuracy: 1.0000\n",
            "Epoch 38/40\n",
            "280/280 [==============================] - 0s 190us/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 7.2741e-04 - val_accuracy: 1.0000\n",
            "Epoch 39/40\n",
            "280/280 [==============================] - 0s 186us/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 6.9623e-04 - val_accuracy: 1.0000\n",
            "Epoch 40/40\n",
            "280/280 [==============================] - 0s 183us/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 6.6758e-04 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZUDERRcxUgL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ca37706-202d-4902-9f12-2318b95ccef0"
      },
      "source": [
        "#Evaluate on Test Data\n",
        "results = model.evaluate(test_examples, test_labels_int)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r21/21 [==============================] - 0s 4ms/sample - loss: 0.0027 - accuracy: 1.0000\n",
            "[0.0026799559127539396, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlXDo1irifeF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def bar_graph(x1,y1):\n",
        "\n",
        "    probabilities = model.predict([x1])\n",
        "\n",
        "    #pred = tf.keras.backend.argmax(probabilities,1)\n",
        "    prediction = np.argmax(probabilities[0])\n",
        "    y=probabilities[0]\n",
        "    N = len(probabilities[0])\n",
        "    x = range(N)\n",
        "    width = 1/1.5\n",
        "    plt.bar(x, y, width, color=\"blue\")\n",
        "    plt.xticks(x,encoder.classes_)\n",
        "    plt.show()\n",
        "    print (\"Input:\",x1)\n",
        "    print (\"Actual:\",encoder.classes_[y1], \" Predicted: \", encoder.classes_[prediction])\n",
        "\n",
        "for i in range(len(test_examples)):\n",
        "  bar_graph(test_examples[i],test_labels_int[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhVPnDjyTAVl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2_wNUX9x8cV"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjOYL0-vx9Nu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "harh8bmwXQ7E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7e5420a2-8d3f-4c35-85da-67fcf6cb46bb"
      },
      "source": [
        "# Other Experiments\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(\"The Quick Brown Fox, Jumps over the lazy dog dog dog dog\")\n",
        "print(tokens)\n",
        "unique_words = set(tokens)\n",
        "print(unique_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['The', 'Quick', 'Brown', 'Fox', ',', 'Jumps', 'over', 'the', 'lazy', 'dog', 'dog', 'dog', 'dog']\n",
            "{'The', 'lazy', 'Quick', 'dog', 'the', 'Brown', 'Fox', 'over', 'Jumps', ','}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}